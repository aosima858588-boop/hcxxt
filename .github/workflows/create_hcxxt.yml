name: Create hcxxt files and PR

on:
  workflow_dispatch:

jobs:
  create_pr:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Create hcxxt files
        run: |
          set -eux
          mkdir -p app/static
          mkdir -p .github/workflows

          cat > README.md <<'EOF'
# hcxxt — 数据导入与在线查询 MVP (FastAPI + Docker)

本项目为最小可用原型（MVP），提供：
- 浏览器上传 CSV 并创建 dataset（每行保存为 JSON record）
- 简单全文/模糊搜索（ILIKE/LIKE）
- 极简前端：上传 / 列表 / 搜索
- 可部署到 Render（或其它容器平台）

API 简要：
- POST /api/upload_csv  (form-data: name, file) — 上传 CSV 并创建 dataset
- GET  /api/datasets  — 列出数据集
- GET  /api/search?dataset_id=1&q=关键词&limit=50&offset=0 — 搜索
- DELETE /api/datasets/{id} — 删除数据集
EOF

          cat > .gitignore <<'EOF'
venv/
__pycache__/
*.pyc
data.db
.env
*.sqlite3
.DS_Store
EOF

          cat > .dockerignore <<'EOF'
__pycache__
*.pyc
venv
.git
.env
EOF

          cat > requirements.txt <<'EOF'
fastapi
uvicorn[standard]
sqlmodel
pandas
python-multipart
aiofiles
psycopg2-binary
EOF

          cat > Dockerfile <<'EOF'
FROM python:3.11-slim

WORKDIR /app

RUN apt-get update && apt-get install -y build-essential libpq-dev gcc --no-install-recommends \
    && rm -rf /var/lib/apt/lists/*

COPY . /app

RUN python -m pip install --upgrade pip
RUN pip install --no-cache-dir -r requirements.txt

ENV PYTHONUNBUFFERED=1
EXPOSE 8000
CMD ["sh", "-c", "uvicorn app.main:app --host 0.0.0.0 --port ${PORT:-8000}"]
EOF

          mkdir -p .github/workflows
          cat > .github/workflows/ci.yml <<'EOF'
name: CI - Build (optional)

on:
  push:
    branches: [ "main", "master" ]

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: checkout
        uses: actions/checkout@v4
      - name: Set up QEMU
        uses: docker/setup-qemu-action@v2
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2
      - name: Build (no push)
        uses: docker/build-push-action@v4
        with:
          context: .
          push: false
          tags: ghcr.io/${{ github.repository }}:hcxxt-latest
EOF

          cat > app/__init__.py <<'EOF'
# package marker for app
EOF

          cat > app/database.py <<'EOF'
import os
from sqlmodel import create_engine, SQLModel

DATABASE_URL = os.environ.get("DATABASE_URL", "sqlite:///./data.db")
connect_args = {"check_same_thread": False} if DATABASE_URL.startswith("sqlite") else {}
engine = create_engine(DATABASE_URL, echo=False, connect_args=connect_args)
def init_db():
    SQLModel.metadata.create_all(engine)
EOF

          cat > app/models.py <<'EOF'
from typing import Optional, Dict, Any
from datetime import datetime
from sqlmodel import SQLModel, Field
from sqlalchemy import Column
try:
    from sqlalchemy.dialects.postgresql import JSONB as _JSON_TYPE
except Exception:
    from sqlalchemy.types import JSON as _JSON_TYPE

class Dataset(SQLModel, table=True):
    id: Optional[int] = Field(default=None, primary_key=True)
    name: str
    created_at: datetime = Field(default_factory=datetime.utcnow)
    columns: Optional[Dict[str, Any]] = Field(default=None, sa_column=Column(_JSON_TYPE))

class Record(SQLModel, table=True):
    id: Optional[int] = Field(default=None, primary_key=True)
    dataset_id: int = Field(index=True)
    data: Dict[str, Any] = Field(sa_column=Column(_JSON_TYPE))
    created_at: datetime = Field(default_factory=datetime.utcnow)
EOF

          cat > app/main.py <<'EOF'
import os, json
from io import BytesIO
from typing import Optional
from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from fastapi.responses import FileResponse
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from sqlmodel import Session, select, text
from .database import engine, init_db
from .models import Dataset, Record
import pandas as pd
from fastapi.encoders import jsonable_encoder

app = FastAPI(title="hcxxt - Data Import & Query")
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

static_dir = os.path.join(os.path.dirname(__file__), "static")
app.mount("/static", StaticFiles(directory=static_dir), name="static")
init_db()

@app.get("/", include_in_schema=False)
def root():
    index_path = os.path.join(static_dir, "index.html")
    if os.path.exists(index_path):
        return FileResponse(index_path, media_type="text/html")
    return {"msg": "hcxxt running"}

@app.post("/api/upload_csv")
async def upload_csv(name: str = Form(...), file: UploadFile = File(...)):
    if not file.filename.lower().endswith(".csv"):
        raise HTTPException(status_code=400, detail="仅支持 CSV 文件")
    content = await file.read()
    try:
        df = pd.read_csv(BytesIO(content))
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"CSV 解析错误: {e}")

    columns = list(df.columns)
    with Session(engine) as session:
        ds = Dataset(name=name, columns={"columns": columns})
        session.add(ds); session.commit(); session.refresh(ds)
        inserted = 0
        for _, row in df.iterrows():
            data = row.to_dict()
            rec = Record(dataset_id=ds.id, data=data)
            session.add(rec); inserted += 1
        session.commit()
    return {"dataset_id": ds.id, "rows": inserted, "columns": columns}

@app.get("/api/datasets")
def list_datasets():
    with Session(engine) as session:
        ds_list = session.exec(select(Dataset)).all()
        out = []
        for d in ds_list:
            res = session.exec(text("SELECT COUNT(*) FROM record WHERE dataset_id = :did"), {"did": d.id}).one()
            count = int(res[0]) if isinstance(res, (list, tuple)) else int(res)
            out.append({"id": d.id, "name": d.name, "created_at": d.created_at, "columns": d.columns, "rows": count})
        return out

@app.get("/api/search")
def search(dataset_id: int, q: Optional[str] = None, limit: int = 50, offset: int = 0):
    with Session(engine) as session:
        if q:
            like_q = f"%{q}%"
            try:
                backend = engine.url.get_backend_name()
            except Exception:
                backend = str(engine.url)
            if "postgres" in backend or "postgresql" in backend:
                sql = text("SELECT id, data FROM record WHERE dataset_id = :did AND data::text ILIKE :like_q LIMIT :lim OFFSET :off")
            else:
                sql = text("SELECT id, data FROM record WHERE dataset_id = :did AND CAST(data AS TEXT) LIKE :like_q LIMIT :lim OFFSET :off")
            rows = session.exec(sql, {"did": dataset_id, "like_q": like_q, "lim": limit, "off": offset}).all()
            results = []
            for r in rows:
                results.append({"id": r[0], "data": jsonable_encoder(r[1])})
            return {"count": len(results), "results": results}
        else:
            sql = text("SELECT id, data FROM record WHERE dataset_id = :did LIMIT :lim OFFSET :off")
            rows = session.exec(sql, {"did": dataset_id, "lim": limit, "off": offset}).all()
            return {"count": len(rows), "results": [{"id": r[0], "data": jsonable_encoder(r[1])} for r in rows]}

@app.delete("/api/datasets/{dataset_id}")
def delete_dataset(dataset_id: int):
    with Session(engine) as session:
        ds = session.get(Dataset, dataset_id)
        if not ds:
            raise HTTPException(status_code=404, detail="dataset not found")
        session.exec(text("DELETE FROM record WHERE dataset_id = :did"), {"did": dataset_id})
        session.delete(ds); session.commit()
        return {"detail": "deleted"}
EOF

          cat > app/static/index.html <<'EOF'



  
  
  


  <h1>hcxxt</h1>
  

  

  




EOF

      - name: Create Pull Request
        uses: peter-evans/create-pull-request@v5
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          branch: add/data-query-mvp
          title: Add data import & query MVP (FastAPI + Docker + Render guide)
          body: Adds a minimal FastAPI-based CSV import and search MVP at repository root, Dockerfile and Render deployment instructions in README.
          base: main
